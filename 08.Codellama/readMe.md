# ğŸ§  Local LLM Code Assistant (Gradio + Ollama)

A lightweight **local AI code assistant** built with **Gradio** and **Ollama**.  
This app allows users to interact with a locally running Large Language Model (LLM) through a simple web interface to generate code, algorithms, and technical explanations â€” all **without external APIs**.

---

## ğŸš€ Features

- âœ… Runs fully **locally** (privacy-friendly)
- ğŸ’» Web UI powered by **Gradio**
- ğŸ§  Supports any **Ollama-compatible model** (e.g., Code LLaMA)
- ğŸ”„ Maintains conversation history for contextual responses
- ğŸ“„ Suitable for long outputs such as code snippets and explanations

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Gradio**
- **Requests**
- **Ollama** (local LLM server)

---

## âš™ï¸ Prerequisites

- Python 3.8+
- Ollama installed and running  
  ğŸ‘‰ https://ollama.com

---

## â–¶ï¸ Installation

1. open command prompt and run: ollama run codeollama
2. ollama create codeguru -f modelfile
3. ollama run codeguru
   
## â–¶ï¸ Run the Application
```bash
python app.py
```

---
## Open your browser and navigate to:
http://127.0.0.1:7860


